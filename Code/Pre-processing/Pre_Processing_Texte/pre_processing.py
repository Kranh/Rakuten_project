# -*- coding: utf-8 -*-
"""pre-processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sudJ7d2M9oip5hjBLW2STLIrBwvkExLv

### **1.   Connexion au compte Google Drive et Importation des bibliothèques de travail"**
"""

# Accès au répertoire Google Drive contenant les fichiers
from google.colab import drive
drive.mount('/content/gdrive')

# Importation de l'extension data_table pour l'affichage optimisé des dataframes
from google.colab import data_table

# Installation des bibliothèques
!pip install ftfy # Déjà installée
!pip install nltk # Déjà installée
!pip install fasttext

# Importation des bibliothèques nécessaires
import pandas as pd
import numpy as np
import fasttext
import re
from bs4 import BeautifulSoup
import ftfy
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

"""### **2.   Lire le fichier "X_train_uptade.csv" et "Y_trainCVw08PX.csv"**"""

# Chargement des fichiers '="X_train_uptade.csv" et "Y_trainCVw08PX.csv"
X_train = pd.read_csv('/content/gdrive/MyDrive/Colab_Notebooks/aug23_cds_rakuten/X_train_update.csv', index_col=0)
y_train = pd.read_csv('/content/gdrive/MyDrive/Colab_Notebooks/aug23_cds_rakuten/Y_train_CVw08PX.csv', index_col=0)

# Afficher le nombre de lignes et de colonnes de chacun des deux fichiers
print("Le nombre de lignes et de colonnes de X_train est de :",X_train.shape,
      "\nLe nombre de lignes et de colonnes de y_train est de :", y_train.shape)

"""### **3.   Fusion des fichiers "X_train_uptade.csv" et "Y_trainCVw08PX.csv"**



"""

# Fusion avec merge des deux datasets
df = pd.merge(X_train, y_train, left_index=True, right_index=True)

"""### **4.   Affichage du DataFrame df**





"""

# Utilsation de l'extension data_table pour afficher le dataFrame df
data_table.DataTable(df, include_index=False, num_rows_per_page=5)

"""### **5.   Taux et gestion des NAs**


"""

# Affichage du nombre de valeurs manquantes
df.isnull().sum() * 100 / len(df)
# Le taux de NAs est de 35,10% pour la colonne "description"

# Remplacement des valeurs NA de la colonne description par la valeur de désignation
df['description'] = df['description'].fillna(df['designation'])
print(df.isnull().sum() * 100 / len(df)) # Affichage du nombre de valeurs manquantes

# Vérifions si les colonnes de type "object" sont des chaînes de caractères
for col in df.columns:
    if df[col].dtype == 'object':
        if df[col].apply(lambda x: isinstance(x, str)).all():
            print(f"La colonne {col} est bien une chaîne de caractères.")
        else:
            print(f"La colonne {col} contient des éléments qui ne sont pas des chaînes de caractères.")

"""### **6.   Supression des balises HTML et des encodages**"""

# Suppression des balises HTML
df['designation'] = df['designation'].apply(lambda text: BeautifulSoup(text, "html.parser").get_text() if text else text)
df['description'] = df['description'].apply(lambda text: BeautifulSoup(text, "html.parser").get_text() if text else text)

# Correction des encodages
df['designation'] = df['designation'].apply(lambda text: ftfy.fix_text(text) if text else text)
df['description'] = df['description'].apply(lambda text: ftfy.fix_text(text) if text else text)

# Afficher les résultats
print(df[['designation', 'description']].head())

"""### **7.   Détection des langues des entrées**

---
"""

model_path = '/content/gdrive/MyDrive/Colab_Notebooks/aug23_cds_rakuten/lid.176.bin'  # Mettez à jour avec le chemin d'accès au modèle de langue FastText
language_model = fasttext.load_model(model_path)

# Fonction pour détecter la langue avec FastText
def detect_language(text):
    # Supprimer les sauts de ligne du texte
    text = text.replace('\n', ' ').replace('\r', '')
    # Prédiction de la langue
    predictions = language_model.predict(text, k=1)  # k=1 pour la meilleure prédiction
    return predictions[0][0].split('__label__')[1]  # Extraire le code de langue

# Appliquer la fonction de détection de langue
df['lang_designation'] = df['designation'].apply(detect_language)
df['lang_description'] = df['description'].apply(detect_language)

# Calculer les pourcentages pour la visualisation
lang_designation_counts = df['lang_designation'].value_counts(normalize=True) * 100
lang_description_counts = df['lang_description'].value_counts(normalize=True) * 100

# Création de l'histogramme pour la colonne 'designation'
plt.figure(figsize=(10, 6))
sns.barplot(x=lang_designation_counts.index, y=lang_designation_counts.values)
plt.title('Pourcentage d\'entrées par langue - Désignation')
plt.xlabel('Langue')
plt.ylabel('Pourcentage d\'entrées (%)')
plt.xticks(rotation=90)  # Pour une meilleure lisibilité des étiquettes de langue
plt.show()

# Création de l'histogramme pour la colonne 'description'
plt.figure(figsize=(10, 6))
sns.barplot(x=lang_description_counts.index, y=lang_description_counts.values)
plt.title('Pourcentage d\'entrées par langue - Description')
plt.xlabel('Langue')
plt.ylabel('Pourcentage d\'entrées (%)')
plt.xticks(rotation=90)  # Pour une meilleure lisibilité des étiquettes de langue
plt.show()

"""### **8.   Recherche de la présence de la désignation dans la description**

---


"""

from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

# Prétraitement léger du texte pour comparaison
def preprocess_for_comparison(text):
    # Convertir en minuscules et supprimer les caractères non-alphabétiques
    return re.sub(r'[^a-z\s]', '', text.lower()) if text else ''

# Fonction pour vérifier si les mots de la désignation sont dans la description
def check_words_in_description(row):
    # Prétraitement des textes
    designation_words = set(preprocess_for_comparison(row['designation']).split())
    description_words = set(preprocess_for_comparison(row['description']).split()) if pd.notna(row['description']) else set()

    # Compter les mots de la désignation présents dans la description
    common_words = designation_words.intersection(description_words)
    return len(common_words) / len(designation_words) if designation_words else 0

# Appliquer la fonction pour chaque ligne
df['designation_in_description_ratio'] = df.apply(check_words_in_description, axis=1)

# Graphe représentant le ratio de présence de la désignation dans la description
# Visualisation du ratio des mots de désignation présents dans la description
sns.histplot(df['designation_in_description_ratio'], bins=50, kde=True)
plt.yscale('log')
plt.title('Répartition du Ratio des Mots de Désignation Présents dans la Description (Échelle Logarithmique)')
plt.xlabel('Ratio de Présence')
plt.ylabel('Logarithme du Nombre de Produits')

# Ajouter une grille mineure pour améliorer la lisibilité sur l'échelle logarithmique
plt.grid(which='minor', axis='y', linestyle='--', linewidth='0.5', color='gray')

# Afficher la figure
plt.show()

"""### **9.   Tokenisation, lemmatisation et supression des stop words français (à modifier pour les langues différentes)**"""

# Téléchargement des ressources NLTK
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
print(stopwords.words('french'))
# Initialiser le lemmatiseur et obtenir les stop words en français, en ajoutant les mots supplémentaires
stop_words = set(stopwords.words('french')) | {"Ap.", "Apr.", "GHz", "MHz", "USD", "a", "afin", "ah", "ai", "aie", "aient", "aies", "ait", "alors", "après", "as", "attendu", "au", "au-delà", "au-devant", "aucun", "aucune", "audit", "auprès", "auquel", "aura", "aurai", "auraient", "aurais", "aurait", "auras", "aurez", "auriez", "aurions", "aurons", "auront", "aussi", "autour", "autre", "autres", "autrui", "aux", "auxdites", "auxdits", "auxquelles", "auxquels", "avaient", "avais", "avait", "avant", "avec", "avez", "aviez", "avions", "avons", "ayant", "ayez", "ayons", "b", "bah", "banco", "ben", "bien", "bé", "c", "c'", "c'est", "c'était", "car", "ce", "ceci", "cela", "celle", "celle-ci", "celle-là", "celles", "celles-ci", "celles-là", "celui", "celui-ci", "celui-là", "celà", "cent", "cents", "cependant", "certain", "certaine", "certaines", "certains", "ces", "cet", "cette", "ceux", "ceux-ci", "ceux-là", "cf.", "cg", "cgr", "chacun", "chacune", "chaque", "chez", "ci", "cinq", "cinquante", "cinquante-cinq", "cinquante-deux", "cinquante-et-un", "cinquante-huit", "cinquante-neuf", "cinquante-quatre", "cinquante-sept", "cinquante-six", "cinquante-trois", "cl", "cm", "cm²", "comme","comprend" "contre", "d", "d'", "d'après", "d'un", "d'une", "dans", "de", "depuis", "derrière", "des", "desdites", "desdits", "desquelles", "desquels", "deux", "devant", "devers", "dg", "différentes", "différents", "divers", "diverses", "dix", "dix-huit", "dix-neuf", "dix-sept", "dl", "dm", "donc", "dont", "douze", "du", "dudit", "duquel", "durant", "dès", "déjà", "e", "eh", "elle", "elles", "en", "en-dehors", "encore", "enfin", "entre", "envers", "es", "est", "et", "eu", "eue", "eues", "euh", "eurent", "eus", "eusse", "eussent", "eusses", "eussiez", "eussions", "eut", "eux", "eûmes", "eût", "eûtes", "f", "fait", "fi", "flac", "fors", "furent", "fus", "fusse", "fussent", "fusses", "fussiez", "fussions", "fut", "fûmes", "fût", "fûtes", "g", "gr", "h", "ha", "han", "hein", "hem", "heu", "hg", "hl", "hm", "hm³", "holà", "hop", "hormis", "hors", "huit", "hum", "hé", "i", "ici", "il", "ils", "j", "j'", "j'ai", "j'avais", "j'étais", "jamais", "je", "jusqu'", "jusqu'au", "jusqu'aux", "jusqu'à", "jusque", "k", "kg", "km", "km²", "l", "l'", "l'autre", "l'on", "l'un", "l'une", "la", "laquelle", "le", "lequel", "les", "lesquelles", "lesquels", "leur", "leurs", "lez", "lors", "lorsqu'", "lorsque", "lui", "lès", "m", "m'", "ma", "maint", "mainte", "maintes", "maints", "mais", "malgré", "me", "mes", "mg", "mgr", "mil", "mille", "milliards", "millions", "ml", "mm", "mm²", "moi", "moins", "mon", "moyennant", "mt", "m²", "m³", "même", "mêmes", "n", "n'avait", "n'y", "ne", "neuf", "ni", "non", "nonante", "nonobstant", "nos", "notre", "nous", "nul", "nulle", "nº", "néanmoins", "o", "octante", "oh", "on", "ont", "onze", "or", "ou", "outre", "où", "p", "par", "par-delà", "parbleu", "parce", "parmi", "pas", "passé", "pendant", "personne", "peu", "plus", "plus_d'un", "plus_d'une", "plusieurs", "pour", "pourquoi", "pourtant", "pourvu", "près", "puisqu'", "puisque", "q", "qu", "qu'", "qu'elle", "qu'elles", "qu'il", "qu'ils", "qu'on", "quand", "quant", "quarante", "quarante-cinq", "quarante-deux", "quarante-et-un", "quarante-huit", "quarante-neuf", "quarante-quatre", "quarante-sept", "quarante-six", "quarante-trois", "quatorze", "quatre", "quatre-vingt", "quatre-vingt-cinq", "quatre-vingt-deux", "quatre-vingt-dix", "quatre-vingt-dix-huit", "quatre-vingt-dix-neuf", "quatre-vingt-dix-sept", "quatre-vingt-douze", "quatre-vingt-huit", "quatre-vingt-neuf", "quatre-vingt-onze", "quatre-vingt-quatorze", "quatre-vingt-quatre", "quatre-vingt-quinze", "quatre-vingt-seize", "quatre-vingt-sept", "quatre-vingt-six", "quatre-vingt-treize", "quatre-vingt-trois", "quatre-vingt-un", "quatre-vingt-une", "quatre-vingts", "que", "quel", "quelle", "quelles", "quelqu'", "quelqu'un", "quelqu'une", "quelque", "quelques", "quelques-unes", "quelques-uns", "quels", "qui", "quiconque", "quinze", "quoi", "quoiqu'", "quoique", "r", "revoici", "revoilà", "rien", "s", "s'", "sa", "sans", "sauf", "se", "seize", "selon", "sept", "septante", "sera", "serai", "seraient", "serais", "serait", "seras", "serez", "seriez", "serions", "serons", "seront", "ses", "si", "sinon", "six", "soi", "soient", "sois", "soit", "soixante", "soixante-cinq", "soixante-deux", "soixante-dix", "soixante-dix-huit", "soixante-dix-neuf", "soixante-dix-sept", "soixante-douze", "soixante-et-onze", "soixante-et-un", "soixante-et-une", "soixante-huit", "soixante-neuf", "soixante-quatorze", "soixante-quatre", "soixante-quinze", "soixante-seize", "soixante-sept", "soixante-six", "soixante-treize", "soixante-trois", "sommes", "son", "sont", "sous", "soyez", "soyons", "suis", "suite", "sur", "sus", "t", "t'", "ta", "tacatac", "tandis", "te", "tel", "telle", "telles", "tels", "tes", "toi", "ton", "toujours", "tous", "tout", "toute", "toutefois", "toutes", "treize", "trente", "trente-cinq", "trente-deux", "trente-et-un", "trente-huit", "trente-neuf", "trente-quatre", "trente-sept", "trente-six", "trente-trois", "trois", "très", "tu", "u", "un", "une", "unes", "uns", "v", "vers", "via", "vingt", "vingt-cinq", "vingt-deux", "vingt-huit", "vingt-neuf", "vingt-quatre", "vingt-sept", "vingt-six", "vingt-trois", "vis-à-vis", "voici", "voilà", "vos", "votre", "vous", "w", "x", "y", "z", "zéro", "à", "ç'", "ça", "ès", "étaient", "étais", "était", "étant", "étiez", "étions", "été", "étée", "étées", "étés", "êtes", "être", "ô"}
lemmatizer = WordNetLemmatizer()
print(stopwords.words('french'))

# Modifier la fonction de prétraitement pour intégrer les nouveaux stop words
def preprocess_text(text):
    # Tokenisation
    tokens = word_tokenize(text.lower())
    # Suppression des stop words et lemmatisation
    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]
    return ' '.join(tokens)

# Appliquer le prétraitement sur les colonnes 'designation' et 'description'
df['designation'] = df['designation'].apply(preprocess_text)
df['description'] = df['description'].apply(preprocess_text)

# Affichage de df
data_table.DataTable(df, include_index=False, num_rows_per_page=5)

"""### **10.   Création d'une nouvelle colonne concaténant designation et description**"""

# Concaténer designation et description directement avec une expression lambda
df['description_complete'] = df.apply(lambda row: row['designation'] + ' ' + row['description']
                                      if pd.notna(row['designation']) and pd.notna(row['description']) and row['designation'] not in row['description']
                                      else (row['description'] if pd.notna(row['description']) else row['designation']), axis=1)

# Affichage du df
#data_table.DataTable(df, include_index=False, num_rows_per_page=5)

"""### **11.   Supression des doublons**"""

# Vérifier les doublons dans la colonne 'description_complete'
duplicates = df['description_complete'].duplicated().sum()

# Afficher le nombre de doublons trouvés
print("Nombre de doublons dans 'description_complete':", duplicates)
# Supprimer les doublons de la colonne 'description_complete'
data = df.drop_duplicates(subset=['description_complete'], keep='first')

# Afficher la taille du DataFrame après suppression des doublons
print("Taille du DataFrame après suppression des doublons :", data.shape)

# Affichage de data
#data_table.DataTable(data, include_index=False, num_rows_per_page=5)

"""### **12.   Détection de la langue de description_complete**

"""

# Appliquer la fonction de détection de langue
data['lang_description_complete'] = data['description_complete'].apply(detect_language)

# Calculer les pourcentages pour la visualisation
lang_description_complete_counts = data['lang_description_complete'].value_counts(normalize=True) * 100

# Création de l'histogramme pour la colonne 'designation'
plt.figure(figsize=(10, 6))
sns.barplot(x=lang_description_complete_counts.index, y=lang_description_complete_counts.values)
plt.title('Pourcentage d\'entrées par langue - Description complète')
plt.xlabel('Langue')
plt.ylabel('Pourcentage d\'entrées (%)')
plt.xticks(rotation=90)  # Pour une meilleure lisibilité des étiquettes de langue
plt.show()

"""### **13.   Graphes de représentation des données**

#### **13.1   Boîte à moustaches**
"""

# Calculer la longueur des ddescriptions completes
data['description_complete_length'] = data['description_complete'].apply(lambda x: len(str(x)))

# Diagramme en boîte de la longueur des descriptions par catégorie de produit
plt.figure(figsize=(15, 5))
sns.boxplot(x='prdtypecode', y='description_complete_length', data=data)
plt.title('Boîte à moustaches de la longueur de la description par catégorie de produit')
plt.xticks(rotation=45)
plt.show()

"""#### **13.2   WordCloud**"""

# Importation de la librairie worcloud et de la fonction WordCloud
from wordcloud import WordCloud

# Fonction pour créer un nuage de mots
def plot_wordcloud(text, ax, title):
    wordcloud = WordCloud(width=800, height=400, background_color='black').generate(text)
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    ax.set_title(title)

# Création des nuages de mots pour chaque catégorie
categories = data['prdtypecode'].unique()
for category in categories:
    category_data = data[data['prdtypecode'] == category]

    # Nuage de mots pour 'description' par catégorie
    fig, ax = plt.subplots(figsize=(10, 5))
    plot_wordcloud(' '.join(category_data['description_complete']), ax, f'Word Cloud for Description (Category {category})')
    plt.show()