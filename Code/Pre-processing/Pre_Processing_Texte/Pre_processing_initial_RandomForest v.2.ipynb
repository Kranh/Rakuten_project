{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBZ7vOVeDkLR"
   },
   "source": [
    "### **1.   Lire le fichier \"X_train_uptade.csv\" et \"Y_trainCVw08PX.csv\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mUUanzVEUPf",
    "outputId": "27d98d2e-2303-4b1d-a0c0-a927782ba1a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de lignes et de colonnes de X_train est de : (84916, 4) \n",
      "Le nombre de lignes et de colonnes de y_train est de : (84916, 1)\n"
     ]
    }
   ],
   "source": [
    "# Importation des librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chargement des fichiers '=\"X_train_uptade.csv\" et \"Y_trainCVw08PX.csv\"\n",
    "X_train = pd.read_csv('../../../../X_train.csv', index_col=0)\n",
    "y_train = pd.read_csv('../../../../Y_train.csv', index_col=0)\n",
    "\n",
    "# Afficher le nombre de lignes et de colonnes de chacun des deux fichiers\n",
    "print(\"Le nombre de lignes et de colonnes de X_train est de :\",X_train.shape,\n",
    "      \"\\nLe nombre de lignes et de colonnes de y_train est de :\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mq0sL_nEH9Jn"
   },
   "source": [
    "### **2.   Fusion des fichiers \"X_train_uptade.csv\" et \"Y_trainCVw08PX.csv\"**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "dQ7lsXBpIT5N"
   },
   "outputs": [],
   "source": [
    "# Fusion avec merge des deux datasets\n",
    "df = pd.merge(X_train, y_train, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45AO419PEcc8"
   },
   "source": [
    "### **3.   Afficher un aperçu des 10 premières lignes**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800
    },
    "id": "x4fSam-xEtMn",
    "outputId": "3b3a3496-9ccd-4020-8c77-e735833f0d59"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN  3804725264  1263597046   \n",
       "1                                                NaN   436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "3                                                NaN    50418756   457047496   \n",
       "4  Luc a des id&eacute;es de grandeur. Il veut or...   278535884  1077757786   \n",
       "\n",
       "   prdtypecode  \n",
       "0           10  \n",
       "1         2280  \n",
       "2           50  \n",
       "3         1280  \n",
       "4         2705  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84911</th>\n",
       "      <td>The Sims [ Import Anglais ]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206719094</td>\n",
       "      <td>941495734</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84912</th>\n",
       "      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n",
       "      <td>&lt;b&gt;Description complète :&lt;/b&gt;&lt;br /&gt;Kit piscine...</td>\n",
       "      <td>3065095706</td>\n",
       "      <td>1188462883</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84913</th>\n",
       "      <td>Journal Officiel De La Republique Francaise N°...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>440707564</td>\n",
       "      <td>1009325617</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84914</th>\n",
       "      <td>Table Basse Bois De Récupération Massif Base B...</td>\n",
       "      <td>&lt;p&gt;Cette table basse a un design unique et con...</td>\n",
       "      <td>3942400296</td>\n",
       "      <td>1267353403</td>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84915</th>\n",
       "      <td>Gomme De Collection 2 Gommes Pinguin Glace Ver...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57203227</td>\n",
       "      <td>684671297</td>\n",
       "      <td>2522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             designation  \\\n",
       "84911                        The Sims [ Import Anglais ]   \n",
       "84912  Kit piscine acier NEVADA déco pierre Ø 3.50m x...   \n",
       "84913  Journal Officiel De La Republique Francaise N°...   \n",
       "84914  Table Basse Bois De Récupération Massif Base B...   \n",
       "84915  Gomme De Collection 2 Gommes Pinguin Glace Ver...   \n",
       "\n",
       "                                             description   productid  \\\n",
       "84911                                                NaN   206719094   \n",
       "84912  <b>Description complète :</b><br />Kit piscine...  3065095706   \n",
       "84913                                                NaN   440707564   \n",
       "84914  <p>Cette table basse a un design unique et con...  3942400296   \n",
       "84915                                                NaN    57203227   \n",
       "\n",
       "          imageid  prdtypecode  \n",
       "84911   941495734           40  \n",
       "84912  1188462883         2583  \n",
       "84913  1009325617         2280  \n",
       "84914  1267353403         1560  \n",
       "84915   684671297         2522  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Affichage des 5 premières lignes\n",
    "display(df.head())\n",
    "\n",
    "# Affichage des 5 dernières lignes\n",
    "display(df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.   Fusion des colonnes Description et Designation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>description_complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "      <td>La Guerre Des Tuques Luc a des id&amp;eacute;es de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productid     imageid  prdtypecode  \\\n",
       "0  3804725264  1263597046           10   \n",
       "1   436067568  1008141237         2280   \n",
       "2   201115110   938777978           50   \n",
       "3    50418756   457047496         1280   \n",
       "4   278535884  1077757786         2705   \n",
       "\n",
       "                                description_complete  \n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...  \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...  \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...  \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...  \n",
       "4  La Guerre Des Tuques Luc a des id&eacute;es de...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer une nouvelle colonne \"description_complete\" en concaténant les valeurs de \"description\" et \"designation\"\n",
    "df['description_complete'] = df['designation'].fillna('') + ' ' + df['description'].fillna('')\n",
    "df.drop(['description', 'designation'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb0o5blnrdve"
   },
   "source": [
    "### **5.   cleaning de description_complete**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for text in df['description_complete'].head():\n",
    "#    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/rts9h4kn0g9ftjd7qn06h2t80000gn/T/ipykernel_18512/403255146.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>description_complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "      <td>La Guerre Des Tuques Luc a des idées de grande...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productid     imageid  prdtypecode  \\\n",
       "0  3804725264  1263597046           10   \n",
       "1   436067568  1008141237         2280   \n",
       "2   201115110   938777978           50   \n",
       "3    50418756   457047496         1280   \n",
       "4   278535884  1077757786         2705   \n",
       "\n",
       "                                description_complete  \n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...  \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...  \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...  \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...  \n",
       "4  La Guerre Des Tuques Luc a des idées de grande...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "df['description_complete'] = df['description_complete'].apply(lambda x: remove_html_tags(x) if pd.notnull(x) else x)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>description_complete</th>\n",
       "      <th>clean_description_complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>olivia personalisiertes notizbuch seiten punkt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>journal art lart marche salon dart asiatique p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>peluche donald europe disneyland marionnette d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "      <td>La Guerre Des Tuques Luc a des idées de grande...</td>\n",
       "      <td>guerre tuques luc ides grandeur veut organiser...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productid     imageid  prdtypecode  \\\n",
       "0  3804725264  1263597046           10   \n",
       "1   436067568  1008141237         2280   \n",
       "2   201115110   938777978           50   \n",
       "3    50418756   457047496         1280   \n",
       "4   278535884  1077757786         2705   \n",
       "\n",
       "                                description_complete  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4  La Guerre Des Tuques Luc a des idées de grande...   \n",
       "\n",
       "                          clean_description_complete  \n",
       "0  olivia personalisiertes notizbuch seiten punkt...  \n",
       "1  journal art lart marche salon dart asiatique p...  \n",
       "2  grand stylet ergonomique bleu gamepad nintendo...  \n",
       "3  peluche donald europe disneyland marionnette d...  \n",
       "4  guerre tuques luc ides grandeur veut organiser...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation de la librairie worcloud et de la fonction WordCloud\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "'''nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')'''\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Fonction pour le prétraitement du texte\n",
    "def preprocess_text(text):\n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    # Supprimer les caractères spéciaux et les chiffres\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenisation\n",
    "    words = word_tokenize(text)\n",
    "    # Créer une liste de stopwords en français\n",
    "    stopwords_french = stopwords.words('french')\n",
    "    # Ajouter \"X\" et \"x\" à la liste de stopwords\n",
    "    stopwords_french.extend([\"Ap.\", \"Apr.\", \"GHz\", \"MHz\", \"USD\", \"a\", \"afin\", \"ah\", \"ai\", \"aie\", \"aient\", \"aies\", \"ait\", \"alors\", \"après\", \"as\", \"attendu\",\n",
    "                             \"au\", \"au-delà\", \"au-devant\", \"aucun\", \"aucune\", \"audit\", \"auprès\", \"auquel\", \"aura\", \"aurai\", \"auraient\", \"aurais\", \"aurait\",\n",
    "                             \"auras\", \"aurez\", \"auriez\", \"aurions\", \"aurons\", \"auront\", \"aussi\", \"autour\", \"autre\", \"autres\", \"autrui\", \"aux\", \"auxdites\", \"auxdits\",\n",
    "                             \"auxquelles\", \"auxquels\", \"avaient\", \"avais\", \"avait\", \"avant\", \"avec\", \"avez\", \"aviez\", \"avions\", \"avons\", \"ayant\", \"ayez\", \"ayons\", \"b\",\n",
    "                             \"bah\", \"banco\", \"ben\", \"bien\", \"bé\", \"c\", \"c'\", \"c'est\", \"c'était\", \"car\", \"ce\", \"ceci\", \"cela\", \"celle\", \"celle-ci\", \"celle-là\", \"celles\",\n",
    "                             \"celles-ci\", \"celles-là\", \"celui\", \"celui-ci\", \"celui-là\", \"celà\", \"cent\", \"cents\", \"cependant\", \"certain\", \"certaine\", \"certaines\", \"certains\",\n",
    "                             \"ces\", \"cet\", \"cette\", \"ceux\", \"ceux-ci\", \"ceux-là\", \"cf.\", \"cg\", \"cgr\", \"chacun\", \"chacune\", \"chaque\", \"chez\", \"ci\", \"cinq\", \"cinquante\", \"cinquante-cinq\",\n",
    "                             \"cinquante-deux\", \"cinquante-et-un\", \"cinquante-huit\", \"cinquante-neuf\", \"cinquante-quatre\", \"cinquante-sept\", \"cinquante-six\", \"cinquante-trois\", \"cl\", \"cm\",\n",
    "                             \"cm²\", \"comme\", \"contre\", \"d\", \"d'\", \"d'après\", \"d'un\", \"d'une\", \"dans\", \"de\", \"depuis\", \"derrière\", \"des\", \"desdites\", \"desdits\", \"desquelles\", \"desquels\",\n",
    "                             \"deux\", \"devant\", \"devers\", \"dg\", \"différentes\", \"différents\", \"divers\", \"diverses\", \"dix\", \"dix-huit\", \"dix-neuf\", \"dix-sept\", \"dl\", \"dm\", \"donc\", \"dont\",\n",
    "                             \"douze\", \"du\", \"dudit\", \"duquel\", \"durant\", \"dès\", \"déjà\", \"e\", \"eh\", \"elle\", \"elles\", \"en\", \"en-dehors\", \"encore\", \"enfin\", \"entre\", \"envers\", \"es\", \"est\",\n",
    "                             \"et\", \"eu\", \"eue\", \"eues\", \"euh\", \"eurent\", \"eus\", \"eusse\", \"eussent\", \"eusses\", \"eussiez\", \"eussions\", \"eut\", \"eux\", \"eûmes\", \"eût\", \"eûtes\", \"f\", \"fait\", \"fi\",\n",
    "                             \"flac\", \"fors\", \"furent\", \"fus\", \"fusse\", \"fussent\", \"fusses\", \"fussiez\", \"fussions\", \"fut\", \"fûmes\", \"fût\", \"fûtes\", \"g\", \"gr\", \"h\", \"ha\", \"han\", \"hein\", \"hem\",\n",
    "                             \"heu\", \"hg\", \"hl\", \"hm\", \"hm³\", \"holà\", \"hop\", \"hormis\", \"hors\", \"huit\", \"hum\", \"hé\", \"i\", \"ici\", \"il\", \"ils\", \"j\", \"j'\", \"j'ai\", \"j'avais\", \"j'étais\", \"jamais\",\n",
    "                             \"je\", \"jusqu'\", \"jusqu'au\", \"jusqu'aux\", \"jusqu'à\", \"jusque\", \"k\", \"kg\", \"km\", \"km²\", \"l\", \"l'\", \"l'autre\", \"l'on\", \"l'un\", \"l'une\", \"la\", \"laquelle\", \"le\", \"lequel\",\n",
    "                             \"les\", \"lesquelles\", \"lesquels\", \"leur\", \"leurs\", \"lez\", \"lors\", \"lorsqu'\", \"lorsque\", \"lui\", \"lès\", \"m\", \"m'\", \"ma\", \"maint\", \"mainte\", \"maintes\", \"maints\", \"mais\",\n",
    "                             \"malgré\", \"me\", \"mes\", \"mg\", \"mgr\", \"mil\", \"mille\", \"milliards\", \"millions\", \"ml\", \"mm\", \"mm²\", \"moi\", \"moins\", \"mon\", \"moyennant\", \"mt\", \"m²\", \"m³\", \"même\", \"mêmes\",\n",
    "                             \"n\", \"n'avait\", \"n'y\", \"ne\", \"neuf\", \"ni\", \"non\", \"nonante\", \"nonobstant\", \"nos\", \"notre\", \"nous\", \"nul\", \"nulle\", \"nº\", \"néanmoins\", \"o\", \"octante\", \"oh\", \"on\", \"ont\",\n",
    "                             \"onze\", \"or\", \"ou\", \"outre\", \"où\", \"p\", \"par\", \"par-delà\", \"parbleu\", \"parce\", \"parmi\", \"pas\", \"passé\", \"pendant\", \"personne\", \"peu\", \"plus\", \"plus_d'un\", \"plus_d'une\",\n",
    "                             \"plusieurs\", \"pour\", \"pourquoi\", \"pourtant\", \"pourvu\", \"près\", \"puisqu'\", \"puisque\", \"q\", \"qu\", \"qu'\", \"qu'elle\", \"qu'elles\", \"qu'il\", \"qu'ils\", \"qu'on\", \"quand\", \"quant\",\n",
    "                             \"quarante\", \"quarante-cinq\", \"quarante-deux\", \"quarante-et-un\", \"quarante-huit\", \"quarante-neuf\", \"quarante-quatre\", \"quarante-sept\", \"quarante-six\", \"quarante-trois\", \"quatorze\",\n",
    "                             \"quatre\", \"quatre-vingt\", \"quatre-vingt-cinq\", \"quatre-vingt-deux\", \"quatre-vingt-dix\", \"quatre-vingt-dix-huit\", \"quatre-vingt-dix-neuf\", \"quatre-vingt-dix-sept\", \"quatre-vingt-douze\",\n",
    "                             \"quatre-vingt-huit\", \"quatre-vingt-neuf\", \"quatre-vingt-onze\", \"quatre-vingt-quatorze\", \"quatre-vingt-quatre\", \"quatre-vingt-quinze\", \"quatre-vingt-seize\", \"quatre-vingt-sept\",\n",
    "                             \"quatre-vingt-six\", \"quatre-vingt-treize\", \"quatre-vingt-trois\", \"quatre-vingt-un\", \"quatre-vingt-une\", \"quatre-vingts\", \"que\", \"quel\", \"quelle\", \"quelles\", \"quelqu'\", \"quelqu'un\",\n",
    "                             \"quelqu'une\", \"quelque\", \"quelques\", \"quelques-unes\", \"quelques-uns\", \"quels\", \"qui\", \"quiconque\", \"quinze\", \"quoi\", \"quoiqu'\", \"quoique\", \"r\", \"revoici\", \"revoilà\", \"rien\", \"s\", \"s'\",\n",
    "                             \"sa\", \"sans\", \"sauf\", \"se\", \"seize\", \"selon\", \"sept\", \"septante\", \"sera\", \"serai\", \"seraient\", \"serais\", \"serait\", \"seras\", \"serez\", \"seriez\", \"serions\", \"serons\", \"seront\", \"ses\", \"si\",\n",
    "                             \"sinon\", \"six\", \"soi\", \"soient\", \"sois\", \"soit\", \"soixante\", \"soixante-cinq\", \"soixante-deux\", \"soixante-dix\", \"soixante-dix-huit\", \"soixante-dix-neuf\", \"soixante-dix-sept\", \"soixante-douze\",\n",
    "                             \"soixante-et-onze\", \"soixante-et-un\", \"soixante-et-une\", \"soixante-huit\", \"soixante-neuf\", \"soixante-quatorze\", \"soixante-quatre\", \"soixante-quinze\", \"soixante-seize\", \"soixante-sept\",\n",
    "                             \"soixante-six\", \"soixante-treize\", \"soixante-trois\", \"sommes\", \"son\", \"sont\", \"sous\", \"soyez\", \"soyons\", \"suis\", \"suite\", \"sur\", \"sus\", \"t\", \"t'\", \"ta\", \"tacatac\", \"tandis\", \"te\", \"tel\",\n",
    "                             \"telle\", \"telles\", \"tels\", \"tes\", \"toi\", \"ton\", \"toujours\", \"tous\", \"tout\", \"toute\", \"toutefois\", \"toutes\", \"treize\", \"trente\", \"trente-cinq\", \"trente-deux\", \"trente-et-un\", \"trente-huit\",\n",
    "                             \"trente-neuf\", \"trente-quatre\", \"trente-sept\", \"trente-six\", \"trente-trois\", \"trois\", \"très\", \"tu\", \"u\", \"un\", \"une\", \"unes\", \"uns\", \"v\", \"vers\", \"via\", \"vingt\", \"vingt-cinq\", \"vingt-deux\",\n",
    "                             \"vingt-huit\", \"vingt-neuf\", \"vingt-quatre\", \"vingt-sept\", \"vingt-six\", \"vingt-trois\", \"vis-à-vis\", \"voici\", \"voilà\", \"vos\", \"votre\", \"vous\", \"w\", \"x\", \"y\", \"z\", \"zéro\", \"à\", \"ç'\", \"ça\", \"ès\",\n",
    "                             \"étaient\", \"étais\", \"était\", \"étant\", \"étiez\", \"étions\", \"été\", \"étée\", \"étées\", \"étés\", \"êtes\", \"être\", \"ô\"])\n",
    "    # Supprimer les stopwords\n",
    "    words = [word for word in words if word not in stopwords_french]\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Prétraiter la colonne description_complete\n",
    "df['clean_description_complete'] = df['description_complete'].apply(preprocess_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2kvCtfbROD9"
   },
   "source": [
    "#### **6 Traduction**\n",
    "La traduction prend trop de temps pour être utilisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Sous-échantillonner la classe majoritaire pour égaler la taille de la classe minoritaire\\nundersampled_majority_data = resample(majority_data, replace=False, n_samples=len(minority_data), random_state=42)\\n\\n# Concaténer les classes minoritaires et sous-échantillonnées\\nundersampled_df = pd.concat([undersampled_majority_data, minority_data])\\n\\n# Afficher les nouvelles proportions\\nprint(undersampled_df['prdtypecode'].value_counts())\\n\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# vectorisation des descriptions des produits\n",
    "vectorizer = TfidfVectorizer(max_features = 5000)  # max_features choisit arbitrairement # Stopwords déjà fait\n",
    "X = vectorizer.fit_transform(df['clean_description_complete']).toarray()\n",
    "y = df['prdtypecode']\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "'''from sklearn.utils import resample\n",
    "\n",
    "# Identifier la classe majoritaire et la classe minoritaire\n",
    "majority_class = df['prdtypecode'].value_counts().idxmax()\n",
    "minority_classes = df['prdtypecode'].value_counts().idxmin()\n",
    "\n",
    "# Séparer les données en classes majoritaires et minoritaires\n",
    "majority_data = df[df['prdtypecode'] == majority_class]\n",
    "minority_data = df[df['prdtypecode'] == minority_classes]'''\n",
    "\n",
    "'''# Sous-échantillonner la classe majoritaire pour égaler la taille de la classe minoritaire\n",
    "undersampled_majority_data = resample(majority_data, replace=False, n_samples=len(minority_data), random_state=42)\n",
    "\n",
    "# Concaténer les classes minoritaires et sous-échantillonnées\n",
    "undersampled_df = pd.concat([undersampled_majority_data, minority_data])\n",
    "\n",
    "# Afficher les nouvelles proportions\n",
    "print(undersampled_df['prdtypecode'].value_counts())\n",
    "'''\n",
    "\n",
    "# Undersampling aléatoire :\n",
    "\n",
    "#from imblearn.under_sampling import RandomUnderSampler,  ClusterCentroids\n",
    "\n",
    "#ru = RandomUnderSampler()\n",
    "#X_ru, y_ru = ru.fit_resample(X_train, y_train)\n",
    "# Afficher les nouvelles proportions\n",
    "#print('Classes échantillon undersampled :', dict(pd.Series(y_ru).value_counts()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction d'un échantillon random (500 lignes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Utiliser stratify pour maintenir la distribution des classes\n",
    "X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Params pour la grid\n",
    "param_grid = {'n_estimators': [10, 50, 100, 200],\n",
    "              'max_features': ['sqrt', 'log2', None],\n",
    "              'min_samples_split': range(2, 30, 2)}\n",
    "\n",
    "# Init RF\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Init Grid\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs = -1)\n",
    "\n",
    "# grid entrainement\n",
    "grid_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Best_params\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7 Vectorization**\n",
    "La plupart des algorithmes de machine learning se basent sur des données numériques. Nous voulons utiliser un random forest, pour ce faire nous avons besoin de convertir les valeurs textes en un format compréhensible par le model.\n",
    "Pour ce faire nous pouvons utiliser TfidfVectorizer disponible dans scikit-learn. TF-IDF (Term Frequency-Inverse Document Frequency) est une technique populaire en machine learning lorsqu'il s'agit de vectorizer des données textuelles en valeurs numériques.\n",
    "\n",
    "- Term Frequency (TF) est la mesure du taux d'apparition d'un mot dans un document.\n",
    "- Inverse Document Frequency (IDF) est la mesure de l'importance d'un mot dans un jeu de données. Les mots présents fréquements dans l'ensemble des documents sont moins important que les mots présents dans peu de documents.\n",
    "- TF-IDF est le produit de TF et de IDF\n",
    "\n",
    "En utilisant TF-IDF nous représentons chaque document comme une vecteur de valeurs numérique où chaque dimencion représente un mot unique du jeu de données. C'est sur cette vectorization que nous allons pouvoir entrainer nos modèles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        1180       0.79      0.98      0.87        45\n",
      "        2583       0.98      0.78      0.87        55\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.88      0.88      0.87       100\n",
      "weighted avg       0.89      0.87      0.87       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # max_features choisit arbitrairement\n",
    "X = vectorizer.fit_transform(df_forest['clean_description_complete']).toarray()\n",
    "y = df_forest['prdtypecode']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'rf__max_features': 'sqrt', 'rf__min_samples_split': 2, 'rf__n_estimators': 200, 'tfidf__max_features': 1000}\n",
      "Best Accuracy: 0.9400000000000001\n"
     ]
    }
   ],
   "source": [
    "'''from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and RandomForestClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "'tfidf__max_features': [1000, 2000, 3000, 5000, 8000],  # [1000, 2000, 3000, 5000, 8000]\n",
    "'rf__n_estimators': [50, 100, 200],  # Nombre d'arbres dans la forêt\n",
    "'rf__max_features': ['sqrt', 'log2', None],  # Nombre de caractéristiques à considérer pour la meilleure division\n",
    "'rf__min_samples_split': range(2, 30, 2)  # Nom du paramètre corrigé\n",
    "}\n",
    "\n",
    "#sqrt Matthieu\n",
    "#log2 Caro\n",
    "#None Tristan\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs = -1) \n",
    "\n",
    "#F1-score\n",
    "classification_report(y_test, y_pred)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(df_forest['clean_description_complete'], df_forest['prdtypecode'])\n",
    "\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
